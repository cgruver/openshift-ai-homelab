apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/recommended-accelerators: '["gpu.intel.com/i915"]'
    opendatahub.io/runtime-version: v2025.3
    openshift.io/display-name: OpenVINO Model Server - Intel ARC
    opendatahub.io/apiProtocol: REST
  labels:
    opendatahub.io/dashboard: "true"
  name: kserve-ovms-intel
spec:
  annotations:
    opendatahub.io/kserve-runtime: ovms
    prometheus.io/path: /metrics
    prometheus.io/port: "8888"
  containers:
    - args:
        - --model_path=/mnt/models
        - --file_system_poll_wait_seconds=0
        - --metrics_enable
      image: registry.redhat.io/rhoai/odh-openvino-model-server-rhel9@sha256:7daebf4a4205b9b81293932a1dfec705f59a83de8097e468e875088996c1f224
      name: kserve-container
      ports:
        - containerPort: 8888
          protocol: TCP
  multiModel: false
  protocolVersions:
    - v2
    - grpc-v2
  supportedModelFormats:
    - autoSelect: true
      name: openvino_ir
      version: opset13

---

apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: qwen3-coder-30b-a3b-instruct
spec:
  predictor:
    model:
      runtime: kserve-ovms-intel
      modelFormat:
        name: openvino_ir
      storageUri: nexus.clg.lab:5002/openvino/qwen3-coder:30b
      args:
        - --model_name=qwen3-coder-30b-a3b-instruct
        - --model_repository_path=/tmp
        - --task=text_generation
        - --enable_prefix_caching=true
        - --tool_parser=qwen3coder 
        - --target_device=GPU
        - --port=8001
        - --rest_port=8888
      resources:
        requests:
          cpu: "100m"
          memory: "16Gi"
          gpu.intel.com/i915: "1"
        limits:
          cpu: "1"
          memory: "32Gi"
          gpu.intel.com/i915: "1"
