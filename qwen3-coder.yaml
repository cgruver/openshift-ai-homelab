apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/recommended-accelerators: '["gpu.intel.com/i915"]'
    opendatahub.io/runtime-version: v2025.3
    openshift.io/display-name: OpenVINO Model Server - Intel ARC
    opendatahub.io/apiProtocol: REST
  labels:
    opendatahub.io/dashboard: "true"
  name: kserve-ovms-intel
spec:
  annotations:
    opendatahub.io/kserve-runtime: ovms
    prometheus.io/path: /metrics
    prometheus.io/port: "8888"
  containers:
    - # image: registry.redhat.io/rhoai/odh-openvino-model-server-rhel9@sha256:7daebf4a4205b9b81293932a1dfec705f59a83de8097e468e875088996c1f224
      image: nexus.clg.lab:5002/openvino/model_server:2025.4.1-gpu
      name: kserve-container
      ports:
        - containerPort: 8888
          protocol: TCP
  multiModel: false
  protocolVersions:
    - v2
    - grpc-v2
  supportedModelFormats:
    - autoSelect: true
      name: openvino_ir

---

apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: qwen3-coder-30b-a3b-instruct
spec:
  predictor:
    model:
      runtime: kserve-ovms-intel
      modelFormat:
        name: openvino_ir
      storageUri: oci://nexus.clg.lab:5002/openvino/qwen3-coder:latest
      args:
        - --config_path=/mnt/models/config_all.json
        - --log_level=DEBUG
        - --port=8001
        - --rest_port=8888
      resources:
        requests:
          cpu: "100m"
          memory: "16Gi"
          gpu.intel.com/i915: "1"
        limits:
          cpu: "1"
          memory: "32Gi"
          gpu.intel.com/i915: "1"
